# im架构演变

## 单体架构

**整体架构**

适用场景：用户规模小、开发迅速

![](http://minhy.top/image/im-%E5%8D%95%E4%BD%93%E6%9E%B6%E6%9E%84.drawio.png)

前后端交互方式，可选项有TCP、UDP、Http、WebSocket。单体架构下选择Http是最简单、最易于实现的方式。

数据存储方面，也是选用Mysql关系型数据库。

**状态维护**

以UID维度设置cookie，并在服务端维护带过期时间的session，实现用户的登录、登出。

由于服务端是采用集群模式，如果nginx根据uid路由到指定机器，那可以在服务端机器上存储session，但是机器重启会导致session失效（重启或者部署，都是很常见的场景），如果能接受该失效场景，成本会更低。因此最好使用集中缓存，例如redis。

**收发消息**

分为两种场景：同时在线、用户离线。

由于Http方式无法实现主动推送消息，消息的接收只能采用定时轮询。

![](http://minhy.top/image/im-%E5%8D%95%E4%BD%93%E6%94%B6%E5%8F%91%E6%B6%88%E6%81%AF.drawio.png)

发送流程：

1. 用户通过Http请求发送消息
2. 服务端将消息同时写入离线消息表、全量消息表，将消息id返回给用户

接收流程：

1. 周期轮询服务端，拉取消息
2. 第一次拉取时，从离线消息表中获取未同步消息
3. 第二次拉取时，携带已同步的消息id请求服务端，服务端从离线消息表中删除已同步消息

**优化方向**

基于周期轮询的消息拉取方式，如果周期短，消息的延迟很低，但对服务端造成的压力较大，如果周期长，消息的延迟很高，用户体验较差。

可以将其优化为基于长连接的Http。

## 分层架构

单体架构只适用于日活较低的场景，随着用户规模逐步扩大，例如几万日活时，需要对架构进行巨大的调整，即对架构进行分层设计。

将服务整体分为四层：APP层、入口网关层、业务逻辑层、数据存储层。

1. APP层：负责与用户进行交互
2. 入口网关层：维护APP与服务端的长连接
3. 业务逻辑层：负责具体业务逻辑，收发消息、鉴权等等
4. 数据存储层

![](http://minhy.top/image/im-%E5%88%86%E5%B1%82%E6%9E%B6%E6%9E%84%E6%95%B4%E4%BD%93%E5%9B%BE.drawio.png)

### APP层

负责与用户进行交互，收发消息，通常会有不同的端类型，web端，各类客户端，承载了端上的业务。

### 入口网关层

作为网关层，主要维护了APP与服务端的长连接。

长连接有两个最重要的数据结构：

1. Map<uid, fd>：需要向用户发送消息时，获取到fd后，基于fd推送消息
2. Map<fd, uid>：fd收到客户端发送的消息后，能获取到uid，知道具体的用户信息

- Uid：客户端的uid
- Fd：客户端与服务端建立的长连接的文件描述符

如果只有网关层只有一台机器，那长连接都维护在本地即可，但这不符合高可用性质，通常网关层也是通过集群部署，满足高可用特性。

集群部署情况下，客户端长连接的具体机器是不确定的，只有两种解决方案：集群间信息互换，节点增加减少时都会带来很大负担；连接状态数据单独集中化存储，例如存储到redis。

redis自身可以随着业务增长进行横向扩展，网关层不用感知到这些信息，用户长连接数量增加时，网关层、redis存储层可以各自独立水平扩展，互不影响。

#### 长连接的心跳检测

1. 遍历扫描：使用map<uid, timeout>存储，检查超时时通过遍历整个map
2. 链表扫描：使用链表有序存储超时时间及其用户信息，尾部是存储最近的长连接信息。鉴抄超时可以说是o(1)，但是获取长连接信息是o(n)的，在此基础上增加一个map用来存储信息，获取长连接信息就变成o(1)了
3. 动态分组：基于链表的方式，每次增加元素时，需要对链表加锁、保证并发不冲突，效率变低。因此按照时间进行分组，增加元素时，只需要对该组进行加锁即可，并发度提高。

### 业务逻辑层

收发消息是核心业务，应该保证核心业务的高性能、高可用。对于一些允许有延迟的业务，可以采用异步消息（MQ）的方式传给下游，而这些业务也不应该影响到核心逻辑。

因此，可以对核心业务、非核心业务进行服务拆分，采用MQ进行通信。